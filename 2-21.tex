\chapter{Learning dynamical systems}
\section{2019-2-21 Kalman filter}

\subsection{Kalman filter}
Consider a LDS with state variable $x_k$, inputs $u_k$, and outputs $z_k$. We assume:
\begin{enumerate}
\item
\begin{align}
x_{k+1} &= F_k x_k + B_k u_k\\
z_k&=H_kx_k + n_k
\end{align}
\item
\begin{align}
w_k &\sim N(0,Q_k)\\
n_k &\sim N(0,R_k).
\end{align}
The Kalman filter is an optimal estimator in this case.
\end{enumerate}
The Kalman filter is computationally and memory efficient.

Given 
\begin{align}
P(x_{k-1}|x_{k-2},z_{k-1})
&\sim N(\wh x_{k-1|k-1}, P_{k-1|k-1})\\
%
p(x_k|x_k,z_k) &\sim N( \wh x_{k|k}, P_{k|k})
\end{align}
and $z_k,u_k$, the Kalman filter can be conceptualized as a 2-step algorithm:
\begin{enumerate}
\item
Prediction: 
\begin{align}
\text{State prediction (a priori estimation)} &&\wh x_{k|k-1} &=F_k \wh x_{k-1|k-1} + B_k u_k\\
\text{Covariance prediction}&&
P_{k|k-1}&=F_k P_{k|k-1} F_k^\top + Q_k.
\end{align}
\item
Update.

We solve a MLE estimation.

Lemma:
\begin{align}
\coltwo xy &\sim N\pa{
\coltwo{\mu_x}{\mu_y}, \matt{\Si_x}{\Si_{xy}}{\Si_{yx}}{\Si_y}
}\\
\mu_{y|x} &= \mu_y + \Si_{yx} \Si_x^{-1}(x-\mu_x)\\
%plus correction term, covariance estimate becomes smaller.
\Si_{y|x} &=\Si_y - \Si_{yx}\Si_x^{-1} \Si_{xy}
\end{align}
The new covariance is the original covariance minus a correction term.

Then the joint distribution is
\begin{align}
\coltwo{x_k}{z_k} &\sim
N\pa{
\coltwo{\wh x_{k|k-1}}{H_k \wh x_{k|k-1}},
\matt{P_{k|k-1}}{P_{k|k-1}H_k^\top}{H_k P_{k|k-1}}{H_k P_{k|k-1}H_k^\top +R_k}
}
\end{align}
\item
The Kalman gain $K_k$ is how confident we are about our observations.
\begin{align}
K_k &= P_{k|k-1} H_k^\top (H_k P_{k|k-1}H_k^\top +R_{k})^{-1}
\end{align}•
\item
State update: 
\begin{align}
\wh x_{k|k} &=\wh x_{k|k-1} + K_k(Z_k-H_k\wh x_{k|k-1}) 
\end{align}
\item
Covariance estimate 
\begin{align}
P_{k|k} &= (I-K_kH_k^\top ) P_{k|k-1}
\end{align}
and find
\begin{align}
\min_{x_k} \rc 2 (x_k - \wh x_{k|k-1})^\top P_{k|k-1}^{-1} (x_k - \wh x_{k|k-1}) + \rc 2(z_k - H_k x_k)^\top R_k^{-1}(z_k - H_k x_k).
\end{align}
The Kalman filter is also called linear quadratic estimation.
\end{enumerate}

I'll talk about 3 extensions: Kalman smoother, high-dimensional data, nonlinear Kalman filter.

%cdl expectation wrt new data.
%
%unsup learning of linear dynamical model
(Kalman filtering is not a machine learning problem. System identification is closer to ML.)

%if no observation, keep doing prediction
%weather forecast
%var of pred value > var of updated value. var reduction?

Applications in control theory: LQG (linear quadratic gaussian) control.
Tracking problems: observations is GPS signal, we observe speed; predict next location. It's not accurate because odometry is not accurate; GPS has a few meters accuracy. Google maps is more accurate than GPS, 1m accurate. When you first open Google maps, you have a big circle representing uncertainty, when you get more observations it shrinks.
Macroeconomic model: can include expectation terms. 

What are the challenges? System identification, nonlinear systems, non-gaussian noise.
Linear gaussian are the simplest assumptions.

Subspace problem: PCA.
LQR: simultaneously learn and control. %How do those methods compare?

You can treat $F_k,B_k$ as state and use a nonlinear filter. This is called an  adaptive filter.



\subsection{Kalman smoothing}

Assume we have observations up to $N>k$, we want to estimate $\wh x_{k|N}$ and $P_{k|N}$. Writing out a huge optimization problem is not efficient. Kalman is a forward propagation; Kalman smoothing is a backwards propagation. Analogue of Viterbi algorithm. (Getting the state is Baum-Welch.)

Given 
\begin{align}
P(x_{k+1}|z_{0:N})
&\sim N(\wh x_{k+1|N} , P_{k-1|N})
\end{align}
and $\wh x_{k|k}, P_{k|k}$, solve
\begin{align}
P(x_k |z_{0:N})&\sim N(\wh x_{k|N}, P_{k|N}).
\end{align}
Treat this as a Kalman filtering problem with $z_k'=x_{k+1}$. 
\begin{align}
\coltwo{x_k}{x_{k+1}}
&\sim 
N\pa{
\coltwo{\wh x_{k|k}}{\wh x_{k+1|N}},
\matt{P_{k|k}}{P_{k|k}F_k^\top}{F_kP_{k|k}}{P_{k+1|k}}
}\\
\wh x_{k|N} &= \wh x_{k|k}+C_k(\wh x_{k+1|N}-\wh x_{k+1|k})\\
\wh P_{k|N} &=P_{k|k} + C_k (P_{k+1|N} - P_{k+1|k})C_k^\top \\
C_k &= P_{k|k}F_k^\top P_{k+1|k}^{-1}
\end{align}
%subopt soln?
%some assms? optimality?
%not solving MLE?

\subsection{High-dimensional data: Ensemble Kalman Filter (EnKF)}

We assume $x_k\in \R^{n\times 1}$, $z_k \in \R^{m\times 1}$. Consider $n\gg m$. Observations are limited, but data are high-dimensional.

This algorithm is empirical. It's widely used in geostatistics. There are several mines; they have limited data from mines. They want to predict petroleum reserves everywhere, like superresolution. Data assimilation.

The expensive part is the multiplication $H_kP_{k|k-1}H_k^\top$, $O(mn^2)$. The inverse has complexity $O(m^3)$. Try to simplify the computation of the Kalman gain $K_k$.
\begin{align}
\wh x_{k|k} &=\wh x_{k|k-1} + K_k(Z_k-H_k\wh x_{k|k-1}) \\
K_k &=P_{k|k-1}
H_k^\top  (H_k P_{k|k-1} H_k^\top + R_k)^{-1}
\end{align}
It's a Monte Carlo-based method. We have particles $X_{k-1|k-1}^{(i)}$ for $i=1,\ldots, s$ for $s\ll n$. The algorithm: Initially sample from the initial gaussian distribution.
\begin{enumerate}
\item
State update
\begin{align}
\wh x_{k|k-1}^{(i)}
&= F_k \wh x_{k-1|k-1}^{(i)} + Bu_k
\end{align}•
\item
\begin{align}
\wh P_{k|k-1} &= \Var(\wh x_{k|k-1})
= \fc{AA^\top}{s-1}\\
A&=[\wh x_{k|k-1}^{(i)} - \E[\wh x_{k|k-1}^{(i)}]
\end{align}
Propagate the samples through time. Replace the matrix in the inverse by $(H_kA) (A^\top H_k)$, in the computation for $K_k$. 
(Think $n\sim 10^7$, $m\sim 10^5$, $s\sim 10^3$.)
%nonlinear weather forecast - why not accurate
\end{enumerate}

Linear problem for which dimensionality high? Most things are nonlinear, do linearization. ``Different copies of weather.''
%Different science proposes different weather models.

Keep track of variance: in linear setting we can compute exact optimal solution. In nonlinear, it's no longer correct. Is using the KF formalism still advantageous? Particle filters are more expensive. EnKF not pure MC; it solves some optimization problem. It's biased but can be close.

In weather forecasting, sometimes they use a nonlinear transition but still use EnKF, it still works to some extent.
Tricks: 
Iterated extended KF, uncertainty transform...

(In order to estimate variance need $s>m$? If $R_k$ is large, not as important.) %I believe you, but as engineer I tell you it's useful.

%modeling, still gaussian

\subsection{Extended Kalman filter (EKF) for nonlinear systems}

%assume we can do linearization
\begin{align}
x_{k+1}&=f(x_k,u_k) + w_k\\
z_k &=h(x_k) + n_k.
\end{align}
The iterated extended Kalman filter (IEKF) has
\begin{align}
\wh x_{k|k-1}
&= f(\wh x_{k|k-1},u_k)\\
P_{k|k-1} &= F_k P_{k-1|k-1}F_k^\top + Q_k,& F_k=\pdd{f}{x}|_{x_k=\wh x_{k|k-1}, u_k = u_k}\\
K_k &= P_kH_k^\top (H_k P_{k|k-1}H_k^\top + R_k)^{-1}\\
\wh x_{k|k}&= \wh x_{k|k-1} + K_k(z_k - h(x_{k|k-1}))\\
P_{k|k}&=(I-K_kH_k^\top)P_{k|k-1}.
\end{align}
%It's like a linearized
%continuous in time analogue
%in nonlinear case, in continuous, still not exact
%substitute $\wh x_{k|k}$ back.

Ex. adaptive optics problems. $x_{k+1}=x_k + f(x_k,u_k)+w_k$, $z_k =x_k^\top x_k + n_k$. %like phase retrieval
$z_k$ intensity observed, $x_k$ electric field. Like phase retrieve. Fourier (nonlinear) optics.
%filtering not ML

Possible directions: 
\begin{enumerate}
\item
When the system is unknown (hidden Markov model, including system identification). Ex. HMM is diffusion process, but don't know the model, what do we learn?
\item
Nonlinear filtering.
\end{enumerate}
%nonlinear filtering ML
%HMM is diffusion process, but don't know the model, what do we learn?
%nonlinear filtering
%MC method
%weather forecasting: ensemble KF
%zakai kushler, pdf, how to solve
%kushni

%it's easy to discuss what's known
%dependence of $f$ on $k$.

%why ML? because difficult to solve. No explicit solution.
